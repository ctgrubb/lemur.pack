---
title: "Brownian Bridges Stuff"
pkgdown:
  as_is: true
output:
  bookdown::html_document2:
    keep_tex: true
    number_sections: true
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##",
  eval = TRUE
)

quick_eval <- FALSE
options(scipen = 6)
set.seed(318937291)
```

```{r libraries, include = FALSE, warning = FALSE, message = FALSE}
library(lemur.pack)
library(MASS)
library(mvtnorm)
library(latex2exp)
library(microbenchmark)
```

# Random Walks

Let $X_1, X_2, X_3, \dots$ be a sequence of i.i.d. random variables with mean 0 and variance 1. Let $S_n := \sum_{i=1}^n X_i$. The stochastic process $S := (S_n)_{n \in \mathbb{N}}$ is known as a **random walk**. 

```{r randomwalk}
n <- 100
X <- rnorm(n)
S <- cumsum(X)
```

```{r randomwalk-fig, echo = FALSE, out.width = "100%", fig.cap = "Example of a random walk showing the first hundred thousand steps"}
par(mar = c(2.5, 2.5, 1, 1))
plot(1:n, S, type = "l", xlab = "n")
```

Now, let 

\[
  W^{(n)}(t) := \frac{S_{\lfloor nt \rfloor}}{\sqrt{n}}, \qquad t \in [0, 1].
\]

The central limit theorem tells us this rescaled $W^{(n)}(1)$ converges in distribution to a standard Gaussian random variable as $n \to \infty$. 

```{r}
t <- seq(0, 1, length = n + 1)[-1]
W <- S[floor(n * t)] / sqrt(n)
```

```{r rescaled-randomwalk-fig, echo = FALSE, out.width = "100%", fig.cap = "The same random walk, but rescaled"}
par(mar = c(2.5, 2.5, 1, 1))
plot(t, W, type = "l", xlab = "t")
```

If we rerun this entire process $k$ times, we can create a QQ plot of $W^{(n)}(1)$.

```{r rescaled-randomwalk-endpoints-fig, echo = FALSE, out.width = "100%", fig.cap = "QQ Plot of 1000 iterations of $W^{(n)}(1)$"}
k <- 1000
Wn <- rep(0, k)

for(i in 1:k) {
  X <- rnorm(n)
  S <- cumsum(X)

  Wn[i] <- S[n] / sqrt(n)
}

par(mar = c(2.5, 2.5, 1, 1))
qqnorm(Wn, main = "")
```

It should come as no surprise that $W^{(n)}(1)$ is in fact distributed standard normal, as 

\[
  W^{(n)}(1) := \frac{1}{\sqrt{n}} \sum_{i=1}^{n} X_i.
\]

# Brownian Bridges

## Construction

A Brownian bridge is the stochastic process $B(t)$ whose probability distribution is the conditional distribution of a standard Wiener process $W(t)$ (also known as Brownian motion) subject to $W(T) = 0$. 

Mathematically, there are a number of ways to arrive at this process. One way is to consider a Wiener process $W_n$ as defined above. Then,

\[
  B(t) := W(t) - \frac{t}{T}W(T)
\]

It is relatively simple to consider a generalized Brownian bridge, where $B(t_1) = a$ and $B(t_2) = b$. We can simulate Brownian bridges using the above method.

```{r bridges}
n <- 5
k <- 100
storage <- matrix(NA, nrow = k, ncol = n)

times <- seq(0, 1, length.out = n + 2)[2:(n + 1)]
target <- 0

for(i in 1:k) {
  dW <- rnorm(n + 1) / sqrt(n)
  W <- cumsum(dW)
  storage[i, ] <- W[1:n] + times * (target - W[n + 1])
}
```

```{r bridge-plot, echo = FALSE, out.width = "100%", fig.cap = "Sample brownian bridges created using rescaling method"}
par(mar = c(2.5, 2.5, 1, 1))
plot(NA, NA, xlim = c(0, 1), ylim = c(-2, 2), xlab = "t (discrete)", ylab = "B(t)")
for(i in 1:k) {
  lines(0:6 / 6, c(0, storage[i, ], 0), type = "l")
}
```

Alternatively, we can return to $S$, as defined above. We can write $S = AX$ where

\[
  A = 
  \begin{bmatrix} 
    1 & 0 & 0 & 0 & \cdots & 0 \\
    1 & 1 & 0 & 0 & \cdots & 0 \\
    1 & 1 & 1 & 0 & \cdots & 0 \\
    1 & 1 & 1 & 1 & \cdots & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & 1 & 1 & 1 & \cdots & 1
  \end{bmatrix}
\]

Thus, $Var(S) = A \Sigma A^T$. In our case, $\Sigma = I$, so $Var(S) = AA^T$, which has the form

\[
  Var(S) = 
  \begin{bmatrix} 
    1 & 1 & 1 & 1 & \cdots & 1 \\
    1 & 2 & 2 & 2 & \cdots & 2 \\
    1 & 2 & 3 & 3 & \cdots & 3 \\
    1 & 2 & 3 & 4 & \cdots & 4 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & 2 & 3 & 4 & \cdots & n
  \end{bmatrix}
\]

The inverse of $AA^T$ is the precision matrix $\Omega$, which also has a distinct form.

\[
  \Omega = 
  \begin{bmatrix} 
    2 & -1 & 0 & 0 & \cdots & 0 \\
    -1 & 2 & -1 & 0 & \cdots & 0 \\
    0 & -1 & 2 & -1 & \cdots & 0 \\
    0 & 0 & -1 & 2 & \cdots & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & 0 & \cdots & 1
  \end{bmatrix}
\]

If we pass this $\Omega$ into the kernel of a Gaussian distribution, the resulting quantity $\exp\{-\frac{1}{2}S^T \Omega S\}$ simplifies.

\begin{align*}
  S^T \Omega S &= 
  \begin{bmatrix} 
    S_1 \\
    S_2 \\
    S_3 \\
    S_4 \\
    \vdots \\
    S_n
  \end{bmatrix}^T
  \begin{bmatrix} 
    2 & -1 & 0 & 0 & \cdots & 0 \\
    -1 & 2 & -1 & 0 & \cdots & 0 \\
    0 & -1 & 2 & -1 & \cdots & 0 \\
    0 & 0 & -1 & 2 & \cdots & 0 \\
    \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & 0 & \cdots & 1
  \end{bmatrix}
  \begin{bmatrix} 
    S_1 \\
    S_2 \\
    S_3 \\
    S_4 \\
    \vdots \\
    S_n
  \end{bmatrix} \\
  &= 
  \begin{bmatrix} 
    2 S_1 - S_2 \\
    -S_1 + 2 S_2 - S_3 \\
    -S_2 + 2 S_3 -S_4 \\
    -S_3 + 2 S_4 - S_5 \\
    \vdots \\
    -S_{n-1} + S_n
  \end{bmatrix}^T
  \begin{bmatrix} 
    S_1 \\
    S_2 \\
    S_3 \\
    S_4 \\
    \vdots \\
    S_n
  \end{bmatrix} \\
  &=
  2 {S_1}^2 - 2 S_1 S_2 + 2 {S_2}^2 - 2 S_2 S_3 + 2 {S_3}^2 - 2 S_3 S_4 + \dots - 2 S_{n-1} S_n + {S_n}^2 \\
  &= (S_1 - 0)^2 + (S_2 - S_1)^2 + (S_3 - S_2)^2 + (S_4 - S_3)^2 + \dots + (S_n - S_{n-1})^2 (\#eq:omega-simplification)
\end{align*}

If we want to tie $S_n$ to 0, is it as simple as adding a $(0 - S_n)^2$ term? This is equivalent to changing the $\Omega_{n,n}$ element to 2 from 1. Also, we can't forget our constant multiplier; we have been working with $S$ but we actually need to work with $W$, so there is an extra $\frac{1}{\sqrt{n}}$ term inside our $A$ matrix.
b
Lets look at an example with $n = 5$. First, we construct $\Omega$.

```{r bridgetest-omega}
omega <- diag(2, n)
omega[abs(row(omega) - col(omega)) == 1] <- -1
```

Now we can construct samples, and plot them. 

```{r bridgetest-samples}
sigma <- 1 / n * chol2inv(chol(omega))
storage <- rmvnorm(k, sigma = sigma)
```

```{r bridgetest-plot, echo = FALSE, out.width = "100%", fig.cap = "Sample brownian bridges created using $\\Omega$ method"}
par(mar = c(2.5, 2.5, 1, 1))
plot(NA, NA, xlim = c(0, 1), ylim = c(-2, 2), xlab = "t (discrete)", ylab = "B(t)")
for(i in 1:k) {
  lines(0:6 / 6, c(0, storage[i, ], 0), type = "l")
}
```

To confirm that these two methods are identical, we can compare the empirical variances at each discrete $t$ value.

```{r bridge-comparison}
n <- 5
k <- 100000
storage <- matrix(NA, nrow = k, ncol = n)

times <- seq(0, 1, length.out = n + 2)[2:(n + 1)]
target <- 0

for(i in 1:k) {
  dW <- rnorm(n + 1) / sqrt(n)
  W <- cumsum(dW)
  storage[i, ] <- W[1:n] + times * (target - W[n + 1])
}

omega <- diag(2, n)
omega[abs(row(omega) - col(omega)) == 1] <- -1

sigma <- 1 / n * chol2inv(chol(omega))
storage2 <- rmvnorm(k, sigma = sigma)

apply(storage, 2, var)
apply(storage2, 2, var)
```

These are very close to each other, so it is not unreasonable to conclude the variances are identical. Since the means are clearly zero, and both methods are normally distributed (first method is linear combination of normals, which is normal), we can conclude they are identically distributed.

## Marginal Distribution

Let $B(t)$ denote a brownian bridge with $t = 1, 2, \dots, T$ discrete steps. If we want the marginal distribution of $B(v)$ for $v \subset t$.

For example, lets look at $k = 100000$ bridges with $4$ intermediate points, and focus on $B(v)$ with $v = 1, 3, 4$. We know the expected value will always be zero, so we focus on the covariance matrix. Empirically, we get

```{r marginal-empirical-sim, echo = FALSE}
n <- 4
k <- 100000

times <- seq(0, 1, length.out = n + 2)[2:(n + 1)]
target <- 0

omega <- diag(2, n)
omega[abs(row(omega) - col(omega)) == 1] <- -1

sigma <- 1 / n * chol2inv(chol(omega))
storage <- rmvnorm(k, sigma = sigma)

var(storage[, c(1, 3, 4)])
```

And theoretically, we should get

```{r marginal-theoretical, echo = FALSE}
fractions(sigma[c(1, 3, 4), c(1, 3, 4)])
```

If we look at the precision matrix instead of covariance (also we will remove the factor of $n$ from now on), we get 

```{r marginal-theoretical-precision-1-1, echo = FALSE}
fractions(solve(n * sigma[c(1, 3, 4), c(1, 3, 4)]))
```

which can be turned into

\[
\begin{align*}
  {x^T \Omega x} &= {x_1}^2 + \frac{1}{2} {x_1}^2 - x_1 x_3 + \frac{3}{2} {x_3}^2 - 2 x_3 x_4 + {x_4}^2 + {x_4}^2  \\
  &= (x_1 - 0)^2 + \frac{1}{2} (x_3 - x_1)^2 + (x_4 - x_3)^2 + (0 - x_4)^2.
\end{align*}
\]


If we instead look at $v = 1, 4$, we get

```{r marginal-theoretical-precision-1-2, echo = FALSE}
fractions(solve(n * sigma[c(1, 4), c(1, 4)]))
```

which can be turned into 

\[
\begin{align*}
  {x^T \Omega x} &= {x_1}^2 + \frac{1}{3} {x_1}^2 - \frac{2}{3} x_1 x_4 + \frac{1}{3} {x_4}^2 + {x_4}^2 \\
  &= (x_1 - 0)^2 + \frac{1}{3} (x_4 - x_1)^2 + (0 - x_4)^2.
\end{align*}
\]

It looks like there is a pattern emerging. To confirm, let $n=6$, and $v = 1, 3, 6$. If we look at the precision matrix instead of covariance (also removing the factor of $n$), we get

```{r marginal-empirical-sim-2, echo = FALSE}
n <- 6
k <- 100000

times <- seq(0, 1, length.out = n + 2)[2:(n + 1)]
target <- 0

omega <- diag(2, n)
omega[abs(row(omega) - col(omega)) == 1] <- -1

sigma <- 1 / n * chol2inv(chol(omega))
storage <- rmvnorm(k, sigma = sigma)
```

```{r marginal-theoretical-precision-2-1, echo = FALSE}
fractions(solve(n * sigma[c(1, 3, 6), c(1, 3, 6)]))
```

which can be turned into 

\[
\begin{align*}
  {x^T \Omega x} &= {x_1}^2 + \frac{1}{2} {x_1}^2 - x_1 x_3 + \frac{5}{6} {x_3}^2 + - \frac{2}{3} x_3 x_6 + \frac{1}{3} {x_6}^2 + {x_4}^2 \\
  &= (x_1 - 0)^2 + \frac{1}{2} (x_3 - x_1)^2 + \frac{1}{3} (x_6 - x_3)^2 + (0 - x_6)^2.
\end{align*}
\]

Note the coefficients are determined by the *distance* between observations. For example, the coefficient in front of $(x_3 - x_1)^2$ is $\frac{1}{2}$ because the distance between $x_3$ and $x_1$ is $2$. Likewise, the coefficient for $(x_6 - x_3)^2$ is $\frac{1}{3}$ because the distance between them is $3$. We can use this fact to create a fast function to construct our tridiagonal precision matrix.

```{r marginal-precision}
marginal.precision <- function(d) {
  obs <- length(d)
  n <- d[obs]
  omega <- matrix(0, nrow = obs, ncol = obs)
  
  omega[1, 1] <- 1 + 1 / (d[2] - d[1])
  omega[obs, obs] <- 1 + 1 / (d[obs] - d[obs - 1])
  
  for(k in 2:(obs - 1)) {
    omega[k, k] <- 1 / (d[k + 1] - d[k]) + 1 / (d[k] - d[k - 1])
  }
  
  for(k in 1:(obs - 1)) {
    omega[1 + k, k] <- omega[k, k + 1] <- -1 / (d[k + 1] - d[k])
  }
  
  return(n * omega)
}
```

## Likelihood Evaluations

Let $b$ be a potential Brownian bridge. If we want to evaluate the log density $f(B = b)$, the most basic way would be to calculate $\Sigma = \Omega^{-1}$ and feed $b$ and $\Sigma$ into a multivariate normal function such as **dmvnorm**. 

```{r denseval-sigma}
denseval.sigma <- function(b) {
  n <- length(b)
  
  omega <- diag(2, n)
  omega[abs(row(omega) - col(omega)) == 1] <- -1
  sigma <- 1 / n * chol2inv(chol(omega))
  
  dmvnorm(b, sigma = sigma, log = TRUE)
}
```

If we are computing the density for many potential bridges, we could calculate $\Sigma$ once, so we should exclude this computation from any comparisons.

```{r denseval-slow}
denseval.slow <- function(b, sigma) {
  dmvnorm(b, sigma = sigma, log = TRUE)
}
```

We can make the calculation faster by exploiting \@ref(eq:omega-simplification). Also, we can make use of the fact that $det(\Omega_{nxn}) = n + 1$. Thus,

\[
  log(f(\mathbf{b}|\mathbf{0}, \mathbf{\Omega})) = \frac{1}{2}\bigg[log(n + 1) + n log(n)- nlog(2\pi) - \mathbf{b}^T\mathbf{\Omega}\mathbf{b}\bigg]
\]

```{r denseval-fast}
denseval.fast <- function(b) {
  n <- length(b)
  d2 <- diff(c(0, b, 0)) ** 2
  1 / 2 * (log(n + 1) + n * log(n) - n * log(2 * pi) - n * sum(d2))
}
```

We can confirm that all versions give the same result.

```{r denseval-check}
n <- 23
omega <- diag(2, n)
omega[abs(row(omega) - col(omega)) == 1] <- -1
sigma <- 1 / n * chol2inv(chol(omega))
b <- rmvnorm(1, sigma = sigma)

denseval.slow(b, sigma)
denseval.fast(b)
```

And we can compare the time it takes for each method. Ignoring the outliers (the way R works often causes the first several evaluations of a function to be much slower), we see that the fast version runs much faster, as expected.

```{r denseval-times, echo = FALSE, out.width = "100%", fig.cap = "Time comparison for methods"}
par(mar = c(2.5, 4.5, 1, 1))

boxplot(microbenchmark(denseval.slow(b, sigma), denseval.fast(b), times = 1000), log = TRUE)
```

## Marginal Likelihood

We can also evaluate the marginal distribution of a subset of points. Let $n = 10$, and we will observe $B(v)$ with $v = 1, 3, 6, 7, 10$. First, lets look empirically at these observations.

```{r marginal-empirical-test}
n <- 10
k <- 100000

times <- seq(0, 1, length.out = n + 2)[2:(n + 1)]
target <- 0

omega <- diag(2, n)
omega[abs(row(omega) - col(omega)) == 1] <- -1

sigma <- 1 / n * chol2inv(chol(omega))
storage <- rmvnorm(k, sigma = sigma)
```

Empirically, if we look at the variance of our select observations, we get

```{r marginal-empirical-test-precision, echo = FALSE}
solve(var(storage[, c(1, 3, 6, 7, 10)]))
# det(solve(var(storage[, c(1, 3, 6, 7, 10)])))
```

If we use our function from above, we get

```{r marginal-theoretical-test-precision, echo = FALSE}
marginal.precision(c(1, 3, 6, 7, 10))
# det.tridiag(marginal.precision(c(1, 3, 6, 7, 10)))
```

And for the determinants we get

```{r precision-det-check}
det(solve(var(storage[, c(1, 3, 6, 7, 10)])))
det.tridiag(marginal.precision(c(1, 3, 6, 7, 10)))
```

Our empirical results match our functions for constructing the precision matrix and calculating the determinant. We can use these to make a function to evaluate the marginal density.

```{r denseval-marginal}
denseval.marginal <- function(b, d) {
  prec <- marginal.precision(d)
  1 / 2 * (log(det.tridiag(prec)) - n * log(2 * pi) - n * t(b) %*% prec %*% b)
}
```

And we can use this function to evaluate the marginal density of various observations.

```{r marginal-dens-test}
dens <- apply(storage, 1, function(x) denseval.marginal(b = x[c(1, 3, 6, 7, 10)], d = c(1, 3, 6, 7, 10)))
```

We can also look at a histogram of $-2 log(B(v))$.

```{r marginal-hist, echo = FALSE, out.width = "100%", fig.cap = "Density of $-2 log(B(v))$, with moment matched Gamma density in red"}
par(mar = c(2.5, 4.5, 1, 1))
logdens <- -2 * dens
maxlogdens <- max(logdens)
newlogdens <- max(logdens) - logdens
plot(density(newlogdens), main = "", xlab = "")
curve(dgamma(x, shape = mean(newlogdens) / (var(newlogdens) / mean(newlogdens)), scale = var(newlogdens) / mean(newlogdens)),
      col = "red", add = TRUE)
```

# Donsker's Theorem

Let $F_n$ be the empirical distribution function of i.i.d. random variables $X_1, X_2, X_3,...$ with distribution function $F$. Define the centered and scaled version of $F_n$ by

\[
G_n(x) = \sqrt{n}(F_n(x) - F(x))
(\#eq:donskerseq)
\]

indexed by $x \in \mathbb{R}$. From the classical central limit theorem for fixed $x$, we know the random variable $G_n(x)$ converges in distribution to a Gaussian random variable with mean zero and variance $F(x)(1 - F(x))$ as the sample size $n$ grows. Donsker's theorem adds to this, and shows that $G_n(x)$ converges in distribution to a Gaussian Process with mean zero and covariance $K(G(s), G(t))$ given by 

\[
K(G(s), G(t)) = E[G(s)G(t)] = min\{F(s), F(t)\} - F(s)F(t)
(\#eq:donskerkernel)
\]

## Continuous Example

Let $x \sim N(0, 1)$. 

First, lets look at some potential CDFs from sampling $X_n$ with $n = 1000$.

```{r normexample, echo = FALSE, out.width = "100%", fig.cap = "Empirical sample CDFs for $X$"}
N <- 10
n <- 1000
storage <- matrix(rnorm(N * n), nrow = N, ncol = n)

colors <- sample(colors()[-1], N)
plot(NA, NA, xlim = c(-3, 3), ylim = c(-0.05, 1.05), xlab = "x", ylab = "F(x)")
for(i in 1:N) {
  lines(sort(storage[i, ]), (1:n) / n, type = "s", col = colors[i])
}
```

Next, lets transform these $F_n(x)$, using Donsker's theorem.

```{r donskerize}
storage2 <- matrix(NA, nrow = N, ncol = n)

for(i in 1:N) {
  storage2[i, ] <- sqrt(n) * (sapply(storage[i, ], function(x) sum(storage[i, ] <= x)) / n - 
                                pnorm(storage[i, ]))
}
```

And now we can visualize them.

```{r donsker-visual, echo = FALSE, out.width = "100%", fig.cap = "Transformed CDF values"}
plot(NA, NA, xlim = c(-3, 3), ylim = c(-1.5, 1.5), xlab = "x", ylab = TeX("$\\sqrt{n}(F_n(x) - F(x))$"))
for(i in 1:N) {
  ord <- order(storage[i, ])
  lines(storage[i, ord], storage2[i, ord], type = "l", col = colors[i])
}
```

Extracting densities.

```{r donsker-densities}
covMat <- function(p){
  outer(p, p, FUN = "pmin") - outer(p, p)
}

denStore <- rep(NA, N)

for(i in 1:N) {
  ord <- order(storage[i, ])
  K <- covMat(pnorm(storage[i, ord]))[-n, -n]
  
  test <- rmvnorm(1, sigma = K)
  dmvnorm(test, sigma = K, log = TRUE)
  
  denStore[i] <- dmvnorm(sqrt(n) * (sapply(storage[i, ord], function(x) sum(storage[i, ] <= x)) / n - pnorm(storage[i, ord]))[-n], sigma = K, log = TRUE)
}

denStore
```

## Discrete Examples

Let's draw a population from  $Unif(\{1, 2, 3, \dots, 100\})$, of size $N = 10000$.

```{r discrete-easy-pop}
N <- 10000
pop <- sample(1:100, size = N, replace = TRUE)
FN <- cumsum(as.numeric(table(pop))) / N
```

### Easy example (large sample)

If we pull large samples, say $n = 1000$, it is almost guaranteed that every potential outcome receives some mass, so we will start with this easy example.

```{r discrete-easy-samples}
k <- 10
n <- 1000
samples <- matrix(sample(pop, size = k * n, replace = TRUE), nrow = k, ncol = n)
```

Now we can calculate our $F_n(x)$ for $x = 1, 2, 3, \dots, 100$.

```{r discrete-easy-cumulative}
Fn <- t(apply(samples, 1, function(x) cumsum(as.numeric(table(x))) / n))
```

And we can visualize them.

```{r discrete-easy-cumulative-plot, echo = FALSE, out.width = "100%", fig.cap = "Sample CDFs"}
colors <- sample(colors()[-1], k)
plot(NA, NA, xlim = c(0, 101), ylim = c(-0.05, 1.05), xlab = "x", ylab = TeX("$F_n(x)$"))
for(i in 1:k) {
  lines(1:100, Fn[i, ], type = "s", col = colors[i])
}
```

The next step is to *Donskerize* these $F_n$ values, by applying the transformation $G_n(x) = \sqrt(n)(F_n(x) - F(x))$.

```{r discrete-donskerize}
Gn <- t(apply(Fn, 1, function(x) sqrt(n) * (x - FN)))
```

And now we can visualize them again.

```{r discrete-donskerized-plot}
plot(NA, NA, xlim = c(0, 101), ylim = c(-1.5, 1.), xlab = "x", ylab = TeX("$G_n(x)$"))
for(i in 1:k) {
  lines(1:100, Gn[i, ], type = "l", col = colors[i])
}
```

And we can get the densities.

```{r discrete-donsker-densities}
# apply(denseval.fast(Gn[1, -100])
```

